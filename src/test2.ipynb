{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 16:55:18.047501: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-06 16:55:18.050571: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-06 16:55:18.061144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741290918.079067   10921 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741290918.084086   10921 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 16:55:18.100930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pprint\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class StyleTransfer:\n",
    "    def __init__(self, content_image, style_image, vgg_model, style_layers=None, content_layer=None, alpha=10, beta=40, img_size=400):\n",
    "        self.content_image = content_image\n",
    "        self.style_image = style_image\n",
    "        self.vgg_model = vgg_model\n",
    "        self.style_layers = style_layers or [\n",
    "            ('block1_conv1', 0.2),\n",
    "            ('block2_conv1', 0.2),\n",
    "            ('block3_conv1', 0.2),\n",
    "            ('block4_conv1', 0.2),\n",
    "            ('block5_conv1', 0.2)]\n",
    "        self.content_layer = content_layer or [('block5_conv4', 1)]\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def get_layer_outputs(self, layer_names):\n",
    "        \"\"\" Creates a vgg model that returns a list of intermediate output values. \"\"\"\n",
    "        outputs = [self.vgg_model.get_layer(layer[0]).output for layer in layer_names]\n",
    "        model = tf.keras.Model([self.vgg_model.input], outputs)\n",
    "        return model\n",
    "\n",
    "    def compute_content_cost(self, content_output, generated_output):\n",
    "        a_C = content_output[-1]\n",
    "        a_G = generated_output[-1]\n",
    "        m, n_H, n_W, n_C = a_G.shape.as_list()\n",
    "\n",
    "        a_C_unrolled = tf.transpose(tf.reshape(a_C, shape=[m, -1, n_C]))\n",
    "        a_G_unrolled = tf.transpose(tf.reshape(a_G, shape=[m, -1, n_C]))\n",
    "\n",
    "        J_content = (1 / (4 * n_H * n_W * n_C)) * tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled)))\n",
    "        return J_content\n",
    "\n",
    "    def compute_layer_style_cost(self, a_S, a_G):\n",
    "        m, n_H, n_W, n_C = a_G.shape\n",
    "        a_S = tf.transpose(tf.reshape(a_S, shape=[-1, n_C]))\n",
    "        a_G = tf.transpose(tf.reshape(a_G, shape=[-1, n_C]))\n",
    "\n",
    "        GS = self.gram_matrix(a_S)\n",
    "        GG = self.gram_matrix(a_G)\n",
    "\n",
    "        J_style_layer = (1 / (4 * n_C ** 2 * (n_H * n_W) ** 2)) * tf.reduce_sum(tf.square(tf.subtract(GS, GG)))\n",
    "        return J_style_layer\n",
    "\n",
    "    def gram_matrix(self, A):\n",
    "        GA = tf.matmul(A, A, transpose_b=True)\n",
    "        return GA\n",
    "\n",
    "    def compute_style_cost(self, style_image_output, generated_image_output):\n",
    "        J_style = 0\n",
    "        a_S = style_image_output[:-1]\n",
    "        a_G = generated_image_output[:-1]\n",
    "        for i, weight in zip(range(len(a_S)), self.style_layers):\n",
    "            J_style_layer = self.compute_layer_style_cost(a_S[i], a_G[i])\n",
    "            J_style += weight[1] * J_style_layer\n",
    "        return J_style\n",
    "\n",
    "    def total_cost(self, J_content, J_style):\n",
    "        J = self.alpha * J_content + self.beta * J_style\n",
    "        return J\n",
    "\n",
    "    def train_step(self, generated_image, a_C, a_S):\n",
    "        with tf.GradientTape() as tape:\n",
    "            a_G = self.vgg_model(generated_image)\n",
    "            J_style = self.compute_style_cost(a_S, a_G)\n",
    "            J_content = self.compute_content_cost(a_C, a_G)\n",
    "            J = self.total_cost(J_content, J_style)\n",
    "\n",
    "        grad = tape.gradient(J, generated_image)\n",
    "        optimizer.apply_gradients([(grad, generated_image)])\n",
    "        generated_image.assign(self.clip_0_1(generated_image))\n",
    "\n",
    "        return J\n",
    "\n",
    "    def clip_0_1(self, image):\n",
    "        return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "\n",
    "    def tensor_to_image(self, tensor):\n",
    "        tensor = tensor * 255\n",
    "        tensor = np.array(tensor, dtype=np.uint8)\n",
    "        if np.ndim(tensor) > 3:\n",
    "            assert tensor.shape[0] == 1\n",
    "            tensor = tensor[0]\n",
    "        return Image.fromarray(tensor)\n",
    "\n",
    "    def generate_image(self, output_path=\"generated_image.jpg\"):\n",
    "        # Preprocess content and style images\n",
    "        content_image = tf.expand_dims(tf.image.convert_image_dtype(self.content_image, tf.float32), axis=0)\n",
    "        style_image = tf.expand_dims(tf.image.convert_image_dtype(self.style_image, tf.float32), axis=0)\n",
    "\n",
    "        # Get the outputs of the layers from the model\n",
    "        vgg_model_outputs = self.get_layer_outputs(self.style_layers + self.content_layer)\n",
    "\n",
    "        # Extract the style and content features from VGG\n",
    "        content_target = vgg_model_outputs(content_image)  # Content encoder\n",
    "        style_targets = vgg_model_outputs(style_image)    # Style encoder\n",
    "\n",
    "        a_C = content_target  # Content features\n",
    "        a_S = style_targets   # Style features\n",
    "\n",
    "        # Initialize generated image with noise added to content image\n",
    "        generated_image = tf.Variable(content_image)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(30):\n",
    "            J = self.train_step(generated_image, a_C, a_S)\n",
    "            print(f\"Epoch {epoch}, Total Cost: {J.numpy()}\")\n",
    "            if epoch % 10 == 0:\n",
    "                image = self.tensor_to_image(generated_image)\n",
    "                image.save(output_path)\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "\n",
    "        return generated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StyleTransfer.__init__() missing 1 required positional argument: 'vgg_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m style_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../media/style.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the style transfer object\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m style_transfer \u001b[38;5;241m=\u001b[39m \u001b[43mStyleTransfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run the style transfer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m style_transfer\u001b[38;5;241m.\u001b[39mgenerate_image(output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_output.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: StyleTransfer.__init__() missing 1 required positional argument: 'vgg_model'"
     ]
    }
   ],
   "source": [
    "# Paths to the content and style images\n",
    "content_image_path = \"../media/content.jpeg\"\n",
    "style_image_path = \"../media/style.jpeg\"\n",
    "\n",
    "# Initialize the style transfer object\n",
    "style_transfer = StyleTransfer(content_image_path, style_image_path, img_size=400, alpha=10, beta=40)\n",
    "\n",
    "# Run the style transfer\n",
    "generated_image = style_transfer.generate_image(output_path=\"generated_output.jpg\")\n",
    "\n",
    "# Display the generated image\n",
    "generated_image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraunhofer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
